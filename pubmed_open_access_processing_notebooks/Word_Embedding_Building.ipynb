{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building word2vec and fasttext models \n",
    "\n",
    "This notebook was used to build the word embedding models. It used the gensim package to build the models. Text is tokenized using Spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenizer for word and sentence tokenization\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "def use_spacy(s, TYPE):\n",
    "    res = nlp(s)\n",
    "    \n",
    "    if TYPE == 'word':\n",
    "        return([t.text for t in res])\n",
    "    if TYPE == 'sent':\n",
    "        return([sent.string.strip() for sent in res.sents])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, which is the corpus as a dict of reports\n",
    "dict_text = pickle.load(open('','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tokenized_doc_list = [use_spacy(v, 'sent') for v in dict_text.values()]\n",
    "sentence_list = [sent for doc in tokenized_doc_list for sent in doc]\n",
    "tokenized_sentences_by_word = [use_spacy(sent, 'word') for sent in sentence_list]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_ft = FastText(tokenized_sentences_by_word, size = 1200, sg = 1, window = 7, seed = 2018,\n",
    "                 min_count = 5, sorted_vocab = 1, min_n = 3, max_n = 8, word_ngrams = 1, workers = 10)\n",
    "model_ft.save('')\n",
    "del(model_ft)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_w2v = Word2Vec(tokenized_sentences_by_word, size = 1200, sg = 1, window = 7, seed = 2018,\n",
    "                    min_count = 5, sorted_vocab = 1, workers = 10)\n",
    "model_w2v.save('') \n",
    "del(model_w2v)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
