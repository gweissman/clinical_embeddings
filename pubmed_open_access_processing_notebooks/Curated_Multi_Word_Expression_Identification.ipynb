{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the curated set of multi-word expressions (MWEs) used in creating our corpora\n",
    "\n",
    "Using both the extrinsic Specialist Lexicon and an intrinsic pointwise mutual information approach we built a currated set of MWEs that we used as a processing step in all of the corpora. MWEs were joined using a _ between terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, nltk, sys\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics.association import QuadgramAssocMeasures\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from flashtext import KeywordProcessor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spacy as a word tokenizer\n",
    "import spacy\n",
    "\n",
    "def use_spacy(s):\n",
    "    res = nlp(s)\n",
    "    return([t.text for t in res])\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-processed text from the PubMed Open Access Case Reports (non-tokenized)\n",
    "dict_text = pickle.load(open(\"\",'rb'))\n",
    "all_text = ' '.join(list(dict_text.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-processed text (tokenized)\n",
    "dict_document_tokenized = pickle.load(open(\"\",'rb'))\n",
    "all_text_tokenized_list = list(dict_document_tokenized.values())\n",
    "all_tokenized_joined = [word for doc in all_text_tokenized_list for word in doc]\n",
    "print('Corpus contains', len(all_tokenized_joined), 'tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specialist lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process the spexialist lexicon\n",
    "nlm_entries_all = open('reference_files/nlm_specialist_lexicon.txt').read().splitlines()\n",
    "nlm_all_terms = [l.split('=')[1] for l in nlm_entries_all if re.match('(^{base=|^spelling_variant=)',l)]\n",
    "nlm_mwes = [t.lower() for t in nlm_all_terms if len(word_tokenize(t)) > 1]\n",
    "print('Found',len(nlm_mwes),'MWEs from the Specialist Lexicon including spelling variants.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_processor = KeywordProcessor()\n",
    "keyword_processor.add_keywords_from_list(nlm_mwes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nlm_mwes = list(set(keyword_processor.extract_keywords(all_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are', len(filtered_nlm_mwes), 'Specialist Lexicon MWEs in the corpus.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identfy multiword phrases- bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_tokenized_joined)\n",
    "bigram_finder.apply_freq_filter(10)\n",
    "all_bigrams_by_pmi = bigram_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_doc_counter = defaultdict(int)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i,doc in enumerate(all_text_tokenized_list):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        sys.stdout.write('completed %d of %d documents \\r' % (i, len(all_text_tokenized_list)))\n",
    "    \n",
    "    bigram_finder = BigramCollocationFinder.from_words(doc)\n",
    "    doc_bigrams_by_pmi = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "    doc_bigrams = [x[0] for x in doc_bigrams_by_pmi]\n",
    "    \n",
    "    for bigram in doc_bigrams:\n",
    "        bigram_doc_counter[bigram] += 1\n",
    "\n",
    "print(\"bigram identification --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_in_more_10_docs = {k:v for k,v in bigram_doc_counter.items() if v > 9}\n",
    "all_bigrams_filtered = [bigram for bigram in all_bigrams_by_pmi if bigram[0] in bigrams_in_more_10_docs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_filtered[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigram_finder = TrigramCollocationFinder.from_words(all_tokenized_joined)\n",
    "trigram_finder.apply_freq_filter(10)\n",
    "all_trigrams_by_pmi = trigram_finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_doc_counter = defaultdict(int)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i,doc in enumerate(all_text_tokenized_list):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        sys.stdout.write('completed %d of %d documents \\r' % (i, len(all_text_tokenized_list)))\n",
    "    \n",
    "    trigram_finder = TrigramCollocationFinder.from_words(doc)\n",
    "    doc_trigrams_by_pmi = trigram_finder.score_ngrams(trigram_measures.pmi)\n",
    "    doc_trigrams = [x[0] for x in doc_trigrams_by_pmi]\n",
    "    \n",
    "    for trigram in doc_trigrams:\n",
    "        trigram_doc_counter[trigram] += 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_in_more_10_docs = {k:v for k,v in trigram_doc_counter.items() if v > 9}\n",
    "all_trigrams_filtered = [trigram for trigram in all_trigrams_by_pmi if trigram[0] in trigrams_in_more_10_docs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trigrams_filtered[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get some output\n",
    "print('Found bigrams:',len(all_bigrams_filtered))\n",
    "print('Found trigrams:',len(all_trigrams_filtered))\n",
    "print('Total n-grams:', (len(all_bigrams_filtered) + len(all_trigrams_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis of identified MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keyword_processor with the final MWEs\n",
    "keyword_processor = KeywordProcessor()\n",
    "keyword_processor.add_keywords_from_list(filtered_nlm_mwes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ngrams_filt = [mwe for mwe in all_bigrams_filtered + all_trigrams_filtered if ' '.join(mwe[0]) not in keyword_processor]\n",
    "print('Identified',len(all_ngrams_filt),'unique MWEs using PMI not found in the Specialist Lexicon.')\n",
    "print('SL terms identified by PMI:', (len(all_bigrams_filtered) + len(all_trigrams_filtered) - len(all_ngrams_filt)), '(', round((len(all_bigrams_filtered) + len(all_trigrams_filtered) - len(all_ngrams_filt)) / len(filtered_nlm_mwes) * 100, 2), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identification of PMI score threshold\n",
    "BI_pmi_mwes_in_SL_scores = np.asarray([mwe[1] for mwe in all_bigrams_filtered if ' '.join(mwe[0]) in keyword_processor])\n",
    "BI_pmi_mwes_NOT_SL_scores = np.asarray([mwe[1] for mwe in all_bigrams_filtered if ' '.join(mwe[0]) not in keyword_processor])\n",
    "BI_pmi_mwes_all_scores = np.asarray([mwe[1] for mwe in all_bigrams_filtered])\n",
    "\n",
    "TRI_pmi_mwes_in_SL_scores = np.asarray([mwe[1] for mwe in all_trigrams_filtered if ' '.join(mwe[0]) in keyword_processor])\n",
    "TRI_pmi_mwes_NOT_SL_scores = np.asarray([mwe[1] for mwe in all_trigrams_filtered if ' '.join(mwe[0]) not in keyword_processor])\n",
    "TRI_pmi_mwes_all_scores = np.asarray([mwe[1] for mwe in all_trigrams_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "ax1 = sns.kdeplot(BI_pmi_mwes_in_SL_scores, label='SL')\n",
    "sns.kdeplot(BI_pmi_mwes_all_scores, ax=ax1, label='not SL')\n",
    "plt.legend()\n",
    "plt.title('Bigrams')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax2 = sns.kdeplot(TRI_pmi_mwes_in_SL_scores, label='SL')\n",
    "sns.kdeplot(TRI_pmi_mwes_all_scores, ax=ax2, label='not SL')\n",
    "plt.legend()\n",
    "plt.title('Trigrams')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter the MWEs identified by PMI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_q50, BI_q95 = np.percentile(BI_pmi_mwes_all_scores, [50, 95])\n",
    "quartile_filtered_bigrams = [mwe for mwe in all_bigrams_filtered if BI_q50 < mwe[1] < BI_q95]\n",
    "print('number filtered bigrams:', len(quartile_filtered_bigrams), '(percent total bigrams:', len(quartile_filtered_bigrams)/len(all_bigrams_filtered), ')')\n",
    "\n",
    "TRI_q50, TRI_q95 = np.percentile(TRI_pmi_mwes_all_scores, [50, 95])\n",
    "quartile_filtered_trigrams = [mwe for mwe in all_trigrams_filtered if TRI_q50 < mwe[1] < TRI_q95]\n",
    "print('number filtered trigrams:', len(quartile_filtered_trigrams), '(percent total trigrams:', len(quartile_filtered_trigrams)/len(all_trigrams_filtered), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join lexicon MWEs and PMI MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_processor = KeywordProcessor()\n",
    "keyword_processor.add_keywords_from_list(filtered_nlm_mwes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now filter\n",
    "non_SL_MWEs = [' '.join(mwe[0]) for mwe in quartile_filtered_bigrams + quartile_filtered_trigrams if ' '.join(mwe[0]) not in keyword_processor]\n",
    "all_mwes = filtered_nlm_mwes + non_SL_MWEs\n",
    "print('Total MWEs from SL and PMI:', len(all_mwes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use KeywordProcessor to replace in whole body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_replacer = KeywordProcessor()\n",
    "\n",
    "mwe_dict = dict(zip(list(map(lambda x: '_'.join(use_spacy(x)), all_mwes)), [[i] for i in all_mwes]))\n",
    "keyword_replacer.add_keywords_from_dict(mwe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keyword_replacer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce both a full text and a dict with MWEs replaced\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "all_text_joined_mwe = keyword_replacer.replace_keywords(all_text)\n",
    "\n",
    "dict_text_MWEs_replaced = {k:keyword_replacer.replace_keywords(v) for k,v in dict_text.items()}\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_joined_mwe[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_text_MWEs_replaced.values())[0][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save MWEs and the corpora with the MWEs joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_mwes, open(\"OA_CR_mwes.pkl\", \"wb\"))\n",
    "pickle.dump(all_text_joined_mwe, open(\"OA_CR_11_1_full_text_with_MWEs.pkl\",\"wb\"))\n",
    "pickle.dump(dict_text_MWEs_replaced, open(\"OA_CR_11_1_doc_dict_with_MWEs.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
