{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of PubMed Open Access Case Reports\n",
    "\n",
    "Case reports in the form of .xml files are cleaned using the text processing helper functions, filtered for length of text and english language text, and stored as a dictionary with PMIDs as keys and text as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, html, pickle, unicodedata, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text processing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for text pre-processing\n",
    "\n",
    "# Pull the contents within the <body> tag\n",
    "def get_body_and_abst_from_xml(raw_xml):\n",
    "    body_xml = re.findall('<body>.*?</body>',raw_xml, re.DOTALL) # re.DOTALL is required to match across line breaks\n",
    "    abstract_xml = re.findall('<abstract.*?>.*?</abstract>', raw_xml, re.DOTALL)\n",
    "    \n",
    "    ## get rid of body and abstract sections that are not in english because some documents have multiple language\n",
    "    ## copies of the case report\n",
    "    if len(body_xml) > 1:\n",
    "        body_xml = [i for i in body_xml if detect(i) == 'en']\n",
    "    if len(abstract_xml) > 1:\n",
    "        abstract_xml = [i for i in abstract_xml if detect(i) == 'en']\n",
    "   \n",
    "\n",
    "    return(' '.join(abstract_xml) + ' '.join(body_xml)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## this could be added\n",
    "    # article_title_xml = re.findall('<article-title>.*?</article-title>', raw_xml, re.DOTALL)\n",
    "    \n",
    "    # need the period to allow for sentence tokenization\n",
    "    # return(' '.join(abstract_xml) + ' '.join(body_xml) + '. '.join(article_title_xml))\n",
    "    \n",
    "    \n",
    "\n",
    "# Removes all tags of the form <x> from the text without any replacement\n",
    "def strip_xml_tags(raw_xml):\n",
    "    tag_def = r'<.*?>'\n",
    "    stripped = re.sub(tag_def, '', raw_xml)\n",
    "    return(stripped)\n",
    "\n",
    "# thankfully Mike Becker found this SO post:\n",
    "# https://stackoverflow.com/questions/2087370/decode-html-entities-in-python-string\n",
    "# Fix representation of unicode characters\n",
    "def fix_unicode_chars(s):\n",
    "    reg = re.compile('&.*?;') #re.compile('&#x.*?;')\n",
    "    matches = re.findall(reg, s)\n",
    "    for m in matches:\n",
    "        s = re.sub(m, html.unescape(m), s)\n",
    "    return(s)\n",
    "\n",
    "# strip newline characters\n",
    "# Removes all newline and tab characters and extra whitespace\n",
    "def strip_breaks_and_tabs(s):\n",
    "    s = re.sub(r'[\\n\\t]',' ', s)\n",
    "    s = re.sub(r'\\s\\s+', ' ', s)\n",
    "    return(s)\n",
    "\n",
    "\n",
    "\n",
    "# remove tags of the form <tag/> that appear in body and break further processing\n",
    "def strip_misplaced_tags(s, tag):\n",
    "    reg = re.compile('<' + tag + '/>')\n",
    "    s = re.sub(reg, '', s)\n",
    "    return(s)\n",
    "\n",
    "\n",
    "# remove contents between a particular tag (don't call this directly -- see below)\n",
    "# Removes an opening tag, its ending, and everything between\n",
    "def strip_tag_and_content(s, tag):\n",
    "    reg = re.compile('\\s?<' + tag + '.*?>.*?</' + tag + '>,?', re.DOTALL)\n",
    "    s = re.sub(reg, '', s)\n",
    "    return(s)\n",
    "\n",
    "# keep list of tags to strip here and do it\n",
    "# Removes anything not in the body of the text, e.g. figure captions, section headers, etc\n",
    "def strip_non_body_text(s):\n",
    "    to_remove = ['title', 'label', 'caption', 'table', 'table-wrap-foot', 'sup', 'xref', 'disp-formula', 'inline-formula']\n",
    "    for tag in to_remove:\n",
    "        s = strip_misplaced_tags(s, tag)\n",
    "        s = strip_tag_and_content(s, tag)\n",
    "    return(s)\n",
    "\n",
    "# sections that have titles and content in p's\n",
    "def strip_section_by_title(s, title):\n",
    "    reg = re.compile('<sec>\\n\\s+' + '<title>' + title + '</title>' + '.*?</sec>', re.DOTALL)\n",
    "    s = re.sub(reg, '', s)\n",
    "    return(s)\n",
    "\n",
    "# now strip the sections\n",
    "def strip_trailing_content(s):\n",
    "    to_remove = [\"competing interests\", \"authors' contributions\", \"pre-publication history\",\n",
    "                \"acknowledgements\", \"acknowledgments\", \"consent\", \"funding\", \"funding sources\",\n",
    "                \"founding sources\", \"conflict of interests\"]\n",
    "    for t in to_remove:\n",
    "        s = strip_section_by_title(s, t)\n",
    "    return(s)\n",
    "\n",
    "# unicode normalize\n",
    "# A unicode normalize\n",
    "def unicode_norm(s):\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    return(s)\n",
    "\n",
    "# replace years with \"year\"\n",
    "# Replace four-digit years with the word year\n",
    "def proc_years(s):\n",
    "    reg = r'\\b(19|20)\\d{2}s?\\b'\n",
    "    s = re.sub(reg, 'year', s)\n",
    "    return(s)\n",
    "\n",
    "# strip citations in square brackets and parentheses\n",
    "# strip citations in square brackets and parentheses\n",
    "# ''' Also remove the space that comes before it'''-- the \\s at the beginning of the reg\n",
    "def strip_citation_numbers(s):\n",
    "    reg = r'\\s[\\[\\(](\\d{1,3}\\s?[,-–]?\\s?)+[\\]\\)]'\n",
    "    s = re.sub(reg, '', s)\n",
    "    # also remove any remaining empty brackets or braces (xrefs...)\n",
    "    s = re.sub(r'\\s(\\[[,-–\\s]?\\]|\\([,-–\\s]?\\))', '', s)\n",
    "    return(s)\n",
    "\n",
    "# remove figure and label references\n",
    "# Remove references to Table 2 or (Fig 1) or (Figure 2)\n",
    "# ''' Also remove the space that comes before it''' -- the \\s at the beginning of the reg\n",
    "def strip_fig_table_refs(s):\n",
    "    reg = re.compile('\\s\\(?(table|figure|fig.?)\\s?\\d?[A-Za-z]?\\)?')\n",
    "    s = re.sub(reg, '', s)\n",
    "    return(s)\n",
    "\n",
    "# convert real numbers to real_number\n",
    "def proc_real_numbers(s):\n",
    "    reg = r'\\d+\\.\\d+'\n",
    "    s = re.sub(reg, 'real_number', s)\n",
    "    return(s)\n",
    "\n",
    "# convert remaining integers to int\n",
    "# Convert integers to \"integer\"\n",
    "def proc_integers(s):\n",
    "    reg = r'\\b\\d+(\\b|\\w)'\n",
    "    s = re.sub(reg, 'integer', s)\n",
    "    return(s)\n",
    "\n",
    "def remove_url(s):\n",
    "    reg = r'^https?:\\/\\/.*[\\r\\n]*\\s?'\n",
    "    s = re.sub(reg, '', s, flags=re.MULTILINE)\n",
    "    return(s)\n",
    "\n",
    "# define aggregate function to do all of the above\n",
    "def preprocess_text(s):\n",
    "    # the order here is crucial: do not change!\n",
    "    s = get_body_and_abst_from_xml(s)\n",
    "    \n",
    "    s = fix_unicode_chars(s)\n",
    "    s = unicode_norm(s)\n",
    "    \n",
    "    s = s.lower()\n",
    "    \n",
    "    s = strip_trailing_content(s)\n",
    "    s = strip_non_body_text(s)\n",
    "    s = strip_xml_tags(s)\n",
    "    s = strip_breaks_and_tabs(s)\n",
    "\n",
    "    s = strip_fig_table_refs(s)\n",
    "    s = strip_citation_numbers(s)\n",
    "    \n",
    "    s = proc_years(s)\n",
    "    \n",
    "    s = remove_url(s)\n",
    "\n",
    "    s = proc_real_numbers(s)    ## skip these for now to keep numbers\n",
    "    s = proc_integers(s)        ## which may contain important information\n",
    "\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process case report xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and process all files\n",
    "# consider do this in parallel with joblib if very slow (about 4 minutes not too bad)\n",
    "dict_text = {} # this data structured will be used for analysis\n",
    "flist = [fname for fname in os.listdir('/Users/zacharyflamholz/Desktop/oa_file_downloads/download_11_1_18/') if not fname.startswith('.')]\n",
    "\n",
    "no_text_reports = []\n",
    "no_english_reports = []\n",
    "\n",
    "for i,fname in enumerate(flist):\n",
    "    \n",
    "    if ((i+1) % 100 == 0):\n",
    "        sys.stdout.write('Completed: %d of %d reports \\r' % (i+1, len(flist)))\n",
    "    \n",
    "    f = open('/Users/zacharyflamholz/Desktop/oa_file_downloads/download_11_1_18/' + fname, 'r', encoding  = 'utf-8')\n",
    "    txt = f.read()\n",
    "    processed_text = preprocess_text(txt)\n",
    "    \n",
    "    \n",
    "    if len(processed_text) < 1:\n",
    "        no_text_reports = no_text_reports + [fname]\n",
    "    else:\n",
    "        try:\n",
    "            if detect(processed_text) == 'en':\n",
    "                dict_text[fname[:-4]] = processed_text\n",
    "        except:\n",
    "            no_english_reports = no_english_reports + [fname]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis of the processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of documents with no text:', len(no_text_reports))\n",
    "print('number of documents with no english text:', len(no_english_reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of spacy and nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the spacy toeknizer\n",
    "\n",
    "# https://github.com/explosion/spaCy/issues/453\n",
    "# https://github.com/explosion/spaCy/issues/1854\n",
    "\n",
    "def use_spacy(s):\n",
    "    res = nlp(s)\n",
    "    return([t.text for t in res])\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "tokenized_dict = {k:use_spacy(v) for k,v in dict_text.items()}\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at how much text we got\n",
    "list_doc_lengths = [len(t) for t in tokenized_dict.values()]\n",
    "plt.hist(list_doc_lengths, bins = 30)\n",
    "plt.xlabel('words')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distrbution of word count for ' + str(len(dict_text)) + ' items.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to filter by some minimum size?\n",
    "word_count_threshold = 100\n",
    "print('There are', len([x for x in list_doc_lengths if x < word_count_threshold]),\n",
    "      'documents with less than', word_count_threshold, 'words.')\n",
    "print('In total the word/token count is:', sum(list_doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# filter\n",
    "dict_text_final = {}\n",
    "tokenized_dict_final = {}\n",
    "\n",
    "for k,v in dict_text.items():\n",
    "    tokenized_doc = use_spacy(v)\n",
    "    if len(tokenized_doc) >= word_count_threshold:\n",
    "        dict_text_final[k] = v\n",
    "        tokenized_dict_final[k] = tokenized_doc\n",
    "\n",
    "        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_doc_lengths_final = [len(v) for v in tokenized_dict_final.values()]\n",
    "print('Documents:',len(dict_text_final))\n",
    "print('Total tokens/Words:',sum(list_doc_lengths_final))\n",
    "print('Median document token count:', np.median(list_doc_lengths_final))\n",
    "print('IQR of document token count:', np.percentile(list_doc_lengths_final, 25), '-',\n",
    "      np.percentile(list_doc_lengths_final, 75))\n",
    "print('Range of token count: ', min(list_doc_lengths_final), '-', max(list_doc_lengths_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all processed text to file\n",
    "# full texts as dict\n",
    "pickle.dump(dict_text_final, open(\"dict_text_OA_CR_Full_Text.pkl\", \"wb\" ))\n",
    "# tokenized texts as dict\n",
    "pickle.dump(tokenized_dict_final, open(\"dict_text_OA_CR_Tokenized.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
